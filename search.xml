<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[rabbitmq故障修复]]></title>
      <url>%2F2017%2F08%2F23%2Frabbitmq%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D%2F</url>
      <content type="text"><![CDATA[故障类型 集群不可用 集群网络分区 消息队列卡死 其他 修复操作集群不可用 问题现象： 三个消息队列服务，其中一个或多个无法启动，service rabbitmq-server start 命令无法拉起服务。 处理方法：重建集群 删除集群缓存文件 12rm -f /var/lib/rabbitmq/erl_crash.dump rm -rf /var/lib/rabbitmq/mnesia/* 重启三个消息队列服务 1service rabbitmq-server restart 选一个主节点，创建集群 1rabbitmqctl start_app 其他节点，加入集群 123rabbitmqctl stop_apprabbitmqctl join_cluster rabbit@node2rabbitmqctl start_app 集群网络分区 问题现象： 在三个节点执行 rabbitmqctl cluster_status 命令，看到节点列表不同 处理方法： 重建集群，方法同上 消息队列消息积压 问题现象： 消息队列执行命令非常卡， rabbitmqctl list_queues 等命令都无法正常输出。 某些队列消息非常多，导致消息积压。 处理方法： 清除消息。 1rabbitmqadmin purge queue name=ceilometer.collector.metering 其他 暂时还没有碰到过其他场景，后面碰到再来补充]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kubernetes学习笔记（一）]]></title>
      <url>%2F2017%2F08%2F21%2Fkubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
      <content type="text"><![CDATA[基本概念 Pod Pod是一组紧密关联的容器集合，它们共享IPC、Network和UTC namespace，是Kubernetes调度的基本单位。Pod的设计理念是支持多个容器在一个Pod中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 包含多个共享IPC、Network和UTC namespace的容器，可直接通过localhost通信 所有Pod内容器都可以访问共享的Volume，可以访问共享数据 Pod一旦调度后就跟Node绑定，即使Node挂掉也不会重新调度，推荐使用Deployments、Daemonsets等控制器来容错 优雅终止：Pod删除的时候先给其内的进程发送SIGTERM，等待一段时间（grace period）后才强制停止依然还在运行的进程 特权容器（通过SecurityContext配置）具有改变系统配置的权限（在网络插件中大量应用） Namespace Namespace是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的pods, services, replication controllers和deployments等都是属于某一个namespace的（默认是default），而node, persistentVolumes等则不属于任何namespace。 Namespace常用来隔离不同的服务，比如Kubernetes自带的服务一般运行在kube-system namespace中，而其他的服务可以运行在用户指定的命名空间中。例如，研发的命名空间（假设为dev），测试的命名空间（假设为test），运维的命名空间（假设为ops）等等，用户可以自行创建和删除命名空间。 Node Node是Pod真正运行的主机，可以物理机，也可以是虚拟机。为了管理Pod，每个Node节点上至少要运行container runtime（比如docker或者rkt）、kubelet和kube-proxy服务。 不像其他的资源（如Pod和Namespace），Node本质上不是Kubernetes来创建的，Kubernetes只是管理Node上的资源。虽然可以通过Manifest创建一个Node对象（如下json所示），但Kubernetes也只是去检查是否真的是有这么一个Node，如果检查失败，也不会往上调度Pod。 默认情况下，kubelet在启动时会向master注册自己，并创建Node资源。 每个Node都包括以下状态信息 地址：包括hostname、外网IP和内网IP 条件（Condition）：包括OutOfDisk、Ready、MemoryPressure和DiskPressure 容量（Capacity）：Node上的可用资源，包括CPU、内存和Pod总数 基本信息（Info）：包括内核版本、容器引擎版本、OS类型等 Service Service是对一组提供相同功能的Pods的抽象，并为它们提供一个统一的入口。借助Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。这些匹配标签的Pod IP和端口列表组成endpoints，由kube-proxy负责将服务IP负载均衡到这些endpoints上。 Service有四种类型： ClusterIP：默认类型，自动分配一个仅cluster内部可以访问的虚拟IP NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过:NodePort来访问该服务 LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到:NodePort ExternalName：将服务通过DNS CNAME记录方式转发到指定的域名（通过spec.externlName设定）。需要kube-dns版本在1.7以上。 Deployment Deployment为Pod和ReplicaSet提供了一个声明式定义(declarative)方法，用来替代以前的ReplicationController来方便的管理应用。 Deployment典型的应用场景包括： 定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment Deployment 为 Pod 和 Replica Set（下一代 Replication Controller）提供声明式更新。 你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。 Deployment 一个典型的用例如下： 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清除旧的不必要的ReplicaSet。 Secret Secret解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。 Secret有三种类型： Opaque：base64编码格式的Secret，用来存储密码、密钥等；但数据也通过base64 –decode解码得到原始数据，所有加密性很弱。 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。 kubernetes.io/service-account-token： 用于被serviceaccount引用。serviceaccout创建时Kubernetes会默认创建对应的secret。Pod如果使用了serviceaccount，对应的secret会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中。 备注： serviceaccount用来使得Pod能够访问Kubernetes API StatefulSet StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计）。 StatefulSet其应用场景包括 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依序进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0） 从上面的应用场景可以发现，StatefulSet由以下几个部分组成： 用于定义网络标志（DNS domain）的Headless Service 用于创建PersistentVolumes的volumeClaimTemplates 定义具体应用的StatefulSet DaemonSet DaemonSet保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。 典型的应用包括： 日志收集，比如fluentd，logstash等 系统监控，比如Prometheus Node Exporter，collectd，New Relic agent，Ganglia gmond等 系统程序，比如kube-proxy, kube-dns, glusterd, ceph等 Service Account Service account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的。 Service Account 与 User account 不同 User account是为人设计的，而service account则是为Pod中的进程调用Kubernetes API而设计； User account是跨namespace的，而service account则是仅局限它所在的namespace； 每个namespace都会自动创建一个default service account Token controller检测service account的创建，并为它们创建secret 开启ServiceAccount Admission Controller后 每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout） 验证Pod引用的service account已经存在，否则拒绝创建 如果Pod没有指定ImagePullSecrets，则把service account的ImagePullSecrets加到Pod中 每个container启动后都会挂载该service account的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/ ReplicationController和ReplicaSet ReplicationController（也简称为rc）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代；而异常多出来的容器也会自动回收。ReplicationController的典型应用场景包括确保健康Pod的数量、弹性伸缩、滚动升级以及应用多版本发布跟踪等。 在新版本的Kubernetes中建议使用ReplicaSet（也简称为rs）来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，并且ReplicaSet支持集合式的selector（ReplicationController仅支持等式）。 虽然也ReplicaSet可以独立使用，但建议使用 Deployment 来自动管理ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如ReplicaSet不支持rolling-update但Deployment支持），并且还支持版本记录、回滚、暂停升级等高级特性。 Job Job负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。 Kubernetes支持以下几种Job： 非并行Job：通常创建一个Pod直至其成功结束 固定结束次数的Job：设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束 带有工作队列的并行Job：设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功 根据.spec.completions和.spec.Parallelism的设置，可以将Job划分为以下几种pattern： Job类型 使用示例 行为 completions Parallelism 一次性Job 数据库迁移 创建一个Pod直至其成功结束 1 1 固定结束次数的Job 处理工作队列的Pod 依次创建一个Pod运行直至completions个成功结束 2+ 1 固定结束次数的并行Job 多个Pod同时处理工作队列 依次创建多个Pod运行直至completions个成功结束 2+ 2+ 并行Job 多个Pod同时处理工作队列 创建一个或多个Pod直至有一个成功结束 1 2+ Horizontal Pod Autoscaling（HPA） Horizontal Pod Autoscaling可以根据CPU使用率或应用自定义metrics自动扩展Pod数量（支持replication controller、deployment和replica set）。 控制管理器每隔30s（可以通过–horizontal-pod-autoscaler-sync-period修改）查询metrics的资源使用情况 支持三种metrics类型 预定义metrics（比如Pod的CPU）以利用率的方式计算 自定义的Pod metrics，以原始值（raw value）的方式计算 自定义的object metrics 支持两种metrics查询方式：Heapster和自定义的REST API 支持多metrics Network Policy 随着微服务的流行，越来越多的云服务平台需要大量模块之间的网络调用。Kubernetes 在 1.3 引入了Network Policy，Network Policy提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。 Ingress 通常情况下，service和pod的IP仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到service在Node上暴露的NodePort上，然后再由kube-proxy通过边缘路由器(edge router)将其转发给相关的Pod或者丢弃。 而Ingress就是为进入集群的请求提供路由规则的集合。ngress可以给service提供集群外部访问的URL、负载均衡、SSL终止、HTTP路由等。为了配置这些Ingress规则，集群管理员需要部署一个Ingress controller，它监听Ingress和service的变化，并根据规则配置负载均衡并提供访问入口。 ConfigMap ConfigMap用于保存配置数据的键值对，可以用来保存单个属性，也可以用来保存配置文件。ConfigMap跟secret很类似，但它可以更方便地处理不包含敏感信息的字符串。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ansible的setup模块可能导致进程卡死]]></title>
      <url>%2F2017%2F01%2F07%2Fansible-setup-module-problem-md%2F</url>
      <content type="text"><![CDATA[问题背景 概述 一次 playbook 的测试过程中，发现一个奇怪的现象：就是执行 playbook 的过程中，总会出现一个 setup 的步骤，并且这个步骤执行的时间非常的长，达到5分钟，甚至有的环境会直接卡死，进程无法中断。详细的问题描述见github-ansible 环境介绍 通过ansible节点管理openstack环境,所以环境中有很多虚拟网卡，导致setup模块的问题。 问题现象 test.yaml 文件 12345---- hosts: xx.xx.xx.xx tasks: - name: test shell: echo aaaa 执行之后，直接卡死，就像下面这样，一直没有返回。 12345# ansible-playbook test.yamlPLAY [xx.xx.xx.xx] ************************************************************TASK [setup] ******************************************************************* 问题分析分别到ansible节点和目标节点查看原因。 ansible节点 1237901 pts/2 Sl+ 2:43 \_ /usr/bin/python /usr/bin/ansible all -m setup7912 pts/2 S+ 0:00 \_ /usr/bin/python /usr/bin/ansible all -m setup7960 pts/2 S+ 0:00 \_ ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r -tt xx.xx.xx.xx /bin/sh -c '/usr/bin/python /root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0' 目标节点 12349036 pts/1 Ss+ 0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0 49083 pts/1 S+ 0:00 \_ /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/setup.py 49213 pts/1 D+ 0:00 \_ /usr/bin/python /tmp/ansible_bx9h6K/ansible_module_setup.py 请注意，这里进程进入了 D 状态，这个状态的意思是不可中断的进程，会一直卡着，无法返回。 setup模块 既然问题出在 setup 模块，那就先来看下setup 模块是干什么的，通过查看源码和文档，这个是获取系统信息，并将相关变量存储到facts中，具体的就不在这里展开分析了。 其中获取网卡信息，过程大概如下123456789101180013 pts/2 Ss+ 0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0 80034 pts/2 S+ 0:00 \_ /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py 80073 pts/2 S+ 0:00 \_ /usr/bin/python /tmp/ansible_pBITNP/ansible_module_setup.py 80092 pts/2 Sl+ 0:25 \_ /usr/bin/ruby /usr/bin/facter --puppet --json 15676 pts/2 R+ 0:00 \_ /sbin/ip link show ovs-system 80013 pts/2 Ss+ 0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0 80034 pts/2 S+ 0:00 \_ /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py 80073 pts/2 S+ 0:00 \_ /usr/bin/python /tmp/ansible_pBITNP/ansible_module_setup.py 80092 pts/2 Sl+ 0:25 \_ /usr/bin/ruby /usr/bin/facter --puppet --json 12971 pts/2 R+ 0:00 \_ /sbin/ip link show ifb992 问题处理openstack的网络节点，会有非常多的虚拟网卡，我的这个环境中虚拟网卡个数接近1000个了，导致收集信息的时间长。既然只是获取信息，那就能否跳过这一个步骤呢？当然是有的，修改配置文件 /etc/ansible/ansible.cfg，其中有一个配置项 gather_subset 1234#原值：#gather_subset=all#新值：gather_subset=!all 修改之后，再次执行playbook，问题得到解决。具体其他的配置项，可以查看setup的源码。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[helloblog]]></title>
      <url>%2F2016%2F12%2F07%2Fhelloblog%2F</url>
      <content type="text"><![CDATA[博客终于开张了！一直想整一个自己的技术博客，最近终于抽出时间整起来了，感谢gitbub,hexo等相关技术贡献者，给大家带来的免费福利，搭建博客的过程很简单。 为什么要搭建自己的博客？ 为了更好的记录和分享 结交更多志同道合的朋友 表达自己最真实的想法 坚持写作，表达，能更好的理解相关内容 虽然以前也有在 CSDN博客 上写过几篇，但是没有坚持下来（当然也有其他的一些原因），这次打算坚持写下去。以前文笔不好，也许写者写着就好起来了呢；以前不喜欢记录和分享，主要还是太懒了，好记性不如烂笔头，自己写出来的东西才是自己理解的，没有明白这个道理，始终只能在入门阶段徘徊，想要成为一代宗师，自己的总结和思考必不可少。 这并不仅仅只是一个技术博客，也许偶尔也会发发牢骚。 好了，写的有点乱，后面坚持写自己的博客！ 最后，以前的 CSDN博客 不再更新，相关文章也会在近期转移到本博客。]]></content>
    </entry>

    
  
  
</search>
