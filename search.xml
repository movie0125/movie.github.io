<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[kubernetes学习笔记（二）]]></title>
      <url>%2F2017%2F09%2F07%2Fkubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
      <content type="text"><![CDATA[kubernetes 核心概念剖析之 Pod 在 Kubernetes 中，pod、replication controller、service 统称为资源，并通过 APIServer 组件提供了增、删、改、查和监控接口。 Pod Pod 是 kubernetes 中能够被创建、调度和管理的最小单元。 Pod 可以想象成一个篮子，而容器则是篮子里的鸡蛋，当 Kubernetes 调度容器时，直接把一个篮子（连同里面的鸡蛋）从一个宿主机调度到另一个宿主机。 篮子和鸡蛋的关系主要表现为一下几点 一个 Pod 里的容器能有多少资源取决于篮子的大小 label 是贴在篮子上 IP 分配给篮子，而不是鸡蛋，篮子里面所有鸡蛋共享这个IP 哪怕只有一个鸡蛋，仍然会分配一个篮子，篮子里也可以鸡蛋 需要强调的是 Pod 里的容器是共享网络和存储的。 从linux namespace 的角度看，pod 里面的容器还共享下面的 namespace: PID namespace，可以看到另一个容器的进程 IPC namespace，能通过 system V IPC 或 POSIX 消息队列进行通信 UTS namespace，共享主机名 为什么需要Pod Pod 是为了解决 “如何合理使用容器支撑企业级复杂应用” 这个问题而诞生的。 试想一下，有一个应用，由三个独立进程组组成，A进程负责响应和处理用户请求，B进程负责转发A产生的日志，C进程负责监控A和B的存货情况并发送定时心跳给监控组件。 在传统的部署方案中，这三个进程被部署到同一个宿主机内。采用容器方案怎么部署呢？ 如果三个进程分别部署到三个容器里面，处理他们之间的关系会非常麻烦，至少需要做到下面几点： B 需要通 A 建立一个共享的 volume C 需要知道 A 和 B 的 PID 还需要维护这些容器之间的关系 任何重新调度，重启或者更新某个容器，都可能破坏容器之间的关系。 labels 和 label selectors 与 pod 协作 当系统中运行者数量庞大的 pod 时，如何有效的分类与组织这些 pod 就成了一个重要的问题。Kubernetes 的解决方案是 labels。每个 pod 都有一个标签，是一组键值对，用来对 pod 进行选择和分类 1234"labels": &#123; "key1": "value1", "key2": "value2"&#125; kubernetes 设计者引入 labels 的主要目的是面相用户，包含 kubernetns 对象功能性和特征描述的 labels 比对象名或者 UUID 更加友好和有意义，而且使用户能够以一种松耦合的方式实现自身组织结构到系统对象之间的映射，无需客户端存储这些映射关系。但是 labels 属性一般不直接作为系统内部唯一识别 kubernetes 对象的依据，因为 labels 并不保证唯一性。一般情况下，允许甚至希望不同的 kubernetes 对象携带一个或一组相同的 labels。 label selector 是 kubernetes 核心的分组机制，通过 label selector，用户能够识别一组有共同特征或属性相同的 kubernetes 对象（后面简称对象 ）。一个 label selector 可以由多个查询条件组成，逗号隔离，这些条件需要同时满足。 pod 的使用场景 下面举例几种使用场景： 一个内容姑那里系统，包括文件和数据加载、本地缓存管理系统的组合 一个常规应用和它的日志和检查点备份、压缩、快照等系统等组合 一个常规应用和它的数据变化检测器，日志实时收集，事件发布等组合 一个常规服务和它的网络代理，桥接和适配器等网络辅助组件的组合 尽量不要在单个 pod 中运行同一个应用的多个实例。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用docker需要考虑的问题]]></title>
      <url>%2F2017%2F09%2F06%2F%E4%BD%BF%E7%94%A8docker%E9%9C%80%E8%A6%81%E8%80%83%E8%99%91%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
      <content type="text"><![CDATA[使用docker需要考虑的问题 学会 docker 基本命令，和一些基本用法之后，进一步需要做什么呢？先把这些问题记录下来，有机会再记录我是怎么处理的。 应用负责均衡，容器IP变化后，负责均衡该怎么处理 应用健康检查怎么处理，比如 web 服务，返回 4XX，5XX,改怎么被感知和处理 如何保证通一个应用的不同容器实例分布在不同的宿主机，或者分布到指定的宿主机 当一个宿主机故障时，节点上面的容器该怎么处理 怎么监控容器内应用的资源使用情况，用来决定是否增加容器集群 如何构建一个 测试-开发-上线 的完整流程 当构建的镜像非常多时，复杂的镜像关系会大大拖延容器创建和启动速度，如何处理这种关系。 大量删除容器，可能导致一些游离的容器，占用大量资源，怎么处理 挂载的 volume 数据该如何进行备份，磁盘写满该怎么处理 宿主机 cpu、内存、磁盘该怎么合理使用]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[容器Link原理解析]]></title>
      <url>%2F2017%2F09%2F01%2F%E5%AE%B9%E5%99%A8Link%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90%2F</url>
      <content type="text"><![CDATA[Link 原理解析 在使用容器部署服务的时候，经常碰到容器之间交互的情况。早期的解决方法是容器向外界进行端口映射的方式实现通信，但是这种方式不够安全，因为提供服务的容器仅仅希望个别容器可以访问。而且这种方式需要经过NAT,效率相对较低。所以 Link（Docker 连接系统） 通信应运而生。Link 可以在两个容器之间建立一个安全的通道，使得接收容器可以通过通道得到源容器指定的相关信息。 Link 使用方法 Link 典型使用场景是 web + db 两个容器通信访问。 12docker run -tid --name db mysql:latestdocker run -tid -P --name web --link db:webdb myweb bash –link 参数的格式 --link &lt;name or id&gt;:alias。其中 name 是容器通过 –name指定或者自动生成的名字；alias 是容器的别名，是接收容器访问源容器的名字。 此例中，webdb 作为接受容器或者父容器，db 作为源容器或者子容器。web 容器可以访问db 容器中的服务。接收容器和源容器的关系是 N:N ，也就是都可以指定多个。 Link 原理 Link 是怎么工作的呢？ 设置接收容器的环境变量 更新接收容器的 /etc/hosts 文件 添加 iptables 规则使容器连接的两个容器可以通信 设置接收容器环境变量 当两个容器通过 --link 建立连接后，会在接收容器中额外设置一些环境变量，用来保存源容器的一些信息，环境变量包括下面两个方面： 每个源容器，接收容器都会设置一个名为 &lt;alias&gt;_NAME 的环境变量，alias 为源容器的别名，如上面的例子， WEBDB_NAME=/web/webdb 源容器中预先设置的环境变量同样会设置在接收容器的环境变量中，比如： Dockerfile 文件中 ENV 设置的环境变量 docker run 命令中 -e --env=[] 设置的环境变量 源容器暴露的端口也会设置到接收容器的环境变量中，比如 db 容器 IP 为 172.17.0.2，对外暴露的端口为 3306，则会在接收容器中设置如下环境变量： WEBDB_PORT_3306_TCP_ADDR=172.17.0.2 WEBDB_PORT_3306_TCP_PORT=3306 WEBDB_PORT_3306_TCP_PROTO=tcp WEBDB_PORT_3306_TCP=tcp://172.17.0.2:3306 WEBDB_PORT=tcp://172.17.0.2:3306前面四个环境变量是为每个暴露的端口设置的，最后一个是所有暴露端口中最小的一个端口的 URL（TCP 优先）。 更新接收容器的 /etc/hosts 文件 Docker 中的 IP 地址是不固定的，容器重启后IP 地址可能就和之前不一样了，虽然设置了环境变量，但是如果源容器重启了，接收容器的环境变量不会自动更新。因此，Link 操作除了将 link 信息保存在环境变量之外，还在 /etc/hosts 文件中添加一项配置 &lt;ip&gt; &lt;alias&gt;，用来解析源容器的 IP 地址。当源容器重启之后，会自动更新接收容器中的 /etc/hosts 文件。因此使用这个 alias 别名来配置应用程序，就不需要担心源容器 IP 的变化。 添加 iptables 规则 设置了环境变量和更新 /etc/hosts 文件之后，接收容器仅仅是得到了源容器的信息，并不代表源容器和接收容器在网络上可以互相通信。为了安全，Docker daemon –icc=false 被设置时，容器间的网络通信是被禁止的。这个时候就需要添加特定的 iptables 规则。 处理过程如下： 得到源容器所有暴露的端口，不仅仅是和主机做了端口映射的端口。 为每一个端口添加 iptables 规则，使得接收容器可以访问。 总结 Link 是一种比端口映射更亲密的容器间通信方式，提供更安全，高效的服务。适合各个组件之间通信的应用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[容器跨主机多子网配置方法（二）]]></title>
      <url>%2F2017%2F08%2F30%2F%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E5%A4%9A%E5%AD%90%E7%BD%91%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
      <content type="text"><![CDATA[容器跨主机多子网配置方法（ overlay + swarm） overlay 是 docker 原生的跨主机多子网方案，其核心是通过 Linux 网桥与 vxlan 隧道实现跨主机划分子网。swarm 是 docker 原生的集群管理方案。 利用 overlay 创建一个网络， docker 会在主机上面创建一个沙盒，实质上就是一个 network namespace（下面简称 netns），在 netns 中创建一个名为 br0 的网桥，并在网桥上面增加一个 vxlan 接口，每个网咯占用一个 vxlan ID。当前使用 Docker 创建的 vxlan ID范围是 256 ~ 1000，因此最多创建 745 个网络。 当容器加入某个网络时，docker 会创建一对网卡设备，一端连接到此网络 netns 中的 br0 网桥上，另一端加入容器的 netns 中，并设置 br0 的IP地址作为容器内路由默认网关地址，从而实现容器加入网络的目的。 实验环境 centos7 1611 Docker version 1.17 VMware 12 在 VMware 里面启动两个centos7 虚拟机，网络选择的是 Net 模式。 实验网络拓扑图 实验前网络拓扑图 实验步骤准备软件 停止防火墙：systemctl stop firewalld 安装docker，并下载docker镜像centos（或者ubuntu都行）。 启动服务：systemctl start docker 镜像准备 由于官方的centos镜像中没有 iproute 等网络工具，需要自己准备一个安装好工具的镜像 网络实验创建 swarm 集群 centos1 执行 12# 初始化 swarm 集群docker swarm init --advertise-addr 192.168.196.128 12345678# 输出如下Swarm initialized: current node (91d9hlqtuafz2e5ri9509lnwq) is now a manager.To add a worker to this swarm, run the following command: docker swarm join --token SWMTKN-1-1qd1i0c769m5vdwqoovl7ql0nzmnh8pfbge7laqj293ovzuii6-0e1erivihld4p4bbvwsi73cg2 192.168.196.128:2377To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. centos2 执行 12345# 根据上面的输出提示，加入 swarm 集群docker swarm join --token SWMTKN-1-1qd1i0c769m5vdwqoovl7ql0nzmnh8pfbge7laqj293ovzuii6-0e1erivihld4p4bbvwsi73cg2 192.168.196.128:2377# 查看 swarm 集群状态docker node ls 创建 overlay 网络 centos1 执行 1234567891011# 创建两个网络 # net1 子网为 192.168.1.0/24# net2 子网为 192.168.2.0/24docker network create -d overlay --subnet=192.168.1.0/24 --attachable net1docker network create -d overlay --subnet=192.168.2.0/24 --attachable net2# 查看所有网络docker network ls# 查看网络详细信息docker network inspect net1 创建容器 centos1 执行，创建容器 container1 和 container2 12docker service create --name container1 --network net1 272c3baad162 bashdocker service create --name container2 --network net2 272c3baad162 bash centos2 执行，创建容器 container3 和 container4 12docker service create --name container3 --network net1 272c3baad162 bashdocker service create --name container4 --network net2 272c3baad162 bash 总结 可惜测试发现这个方式不成功，有 bug 没有解决，有机会等问题解决后在测试下。 github地址 参考文献 官网overlay-swarm]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[容器跨主机多子网配置方法（一）]]></title>
      <url>%2F2017%2F08%2F29%2F%E5%AE%B9%E5%99%A8%E8%B7%A8%E4%B8%BB%E6%9C%BA%E5%A4%9A%E5%AD%90%E7%BD%91%E9%85%8D%E7%BD%AE%E6%96%B9%E6%B3%95%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
      <content type="text"><![CDATA[容器跨主机多子网配置方法（ openvswitch + docker + namespace ） 本实验中使用 Openvswitch 网桥替换了 docker 默认的 linux 网桥。Openswitch 是一个开源的虚拟交换机的实现，在云计算领域有广泛的应用，利用 Openvswitch 可以实现更加丰富和强大的网络功能。 本实验按照下面的网络拓扑结构，搭建两个跨主机的 vlan, 主机 centos1 运行两个容器 container1 和 container2, 分别属于 vlan1 和 vlan2， 主机 centos2 同样运行两个容器 container3 和 container4，分别属于 vlan1 和 vlan2。 最终，vlan1 内部的两个容器可以互相通信，vlan2 内部的两个容器也可以通信，但是 vlan1 和 vlan2 互相隔离，互相不可访问。 实验环境 centos7 1611 Docker version 1.12.6 VMware 12 在 VMware 里面启动两个centos7 虚拟机，网络选择的是 Net 模式。 实验网络拓扑图 实验前网络拓扑图 实验后网络拓扑图 实验步骤准备软件 安装 openvswitch openswitch下载 安装docker，并下载docker镜像centos（或者ubuntu都行）。 启动服务：systemctl start docker openvswitch 镜像准备 由于官方的centos镜像中没有 iproute 等网络工具，需要自己准备一个安装好工具的镜像 准备网络 centos1 操作 123456789101112131415# 创建 ovs 网桥ovs-vsctl add-br ovs-br0# 创建一个到 centos2 的 vxlan tunnelovs-vsctl add-port ovs-br0 vxlan-to-centos2 -- set interface vxlan-to-centos2 type=vxlan option:remote_ip="192.168.196.129"# 由于虚拟机的 IP 原先是在 ens33 网卡上# 为了让两个虚拟机的网桥之间能通信# 需要吧 ens33 网卡绑定到网桥 ovs-br0# 并且把 ens33 网卡上面的 IP 配置到网桥 ovs-br0 上ip addr del 192.168.196.128/24 dev ens33ip addr add 192.168.196.128/24 dev ovs-br0ovs-vsctl add-port ovs-br0 ens33# 如果目录不存在需要先创建mkdir /var/run/netns centos2 操作 12345678910# 创建 ovs 网桥ovs-vsctl add-br ovs-br0# 创建一个到 centos1 的 vxlan tunnelovs-vsctl add-port ovs-br0 vxlan-to-centos1 -- set interface vxlan-to-centos1 type=vxlan option:remote_ip="192.168.196.128"ip addr del 192.168.196.129/24 dev ens33ip addr add 192.168.196.129/24 dev ovs-br0ovs-vsctl add-port ovs-br0 ens33# 如果目录不存在需要先创建mkdir /var/run/netns 创建容器container1 和 container2 在 centos1 操作 container1 操作 12345678910111213141516171819202122# 创建容器 --net=none 表示这个容器没有网卡设备，272c3baad162 表示我自己准备的 centos镜像IDdocker run -tid --net=none --name=container1 272c3baad162 /bin/bashpid=$(docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' container1)echo $pid# 在 /var/run/netns 目录下面创建可以使用 ip netns 命令操作的 namespaceln -s /proc/$pid/ns/net /var/run/netns/$pid# 为container1 创建一对虚拟网卡ip link add name vethC1Host mtu 1500 type veth peer name vethC1Container mtu 1500# veth pair 的一端加入创建的 OVS 网桥中，设置 VLAN 为 1ovs-vsctl add-port ovs-br0 vethC1Host tag=1ip link set vethC1Host up# veth pair 的另一端加入容器所在 namespaceip link set vethC1Container netns $pid# 进入 $pid 所在 netns 中，配置刚刚放入的网卡，改名为 eth0，配置 IP 并启用ip netns exec $pid ip link set dev vethC1Container name eth0ip netns exec $pid ip addr add 192.168.0.1/24 dev eth0ip netns exec $pid ip link set eth0 up container2 操作 12345678910111213141516171819202122# 创建容器 --net=none 表示这个容器没有网卡设备，272c3baad162 表示我自己准备的 centos镜像IDdocker run -tid --net=none --name=container2 272c3baad162 /bin/bashpid=$(docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' container2)echo $pid# 在 /var/run/netns 目录下面创建可以使用 ip netns 命令操作的 namespaceln -s /proc/$pid/ns/net /var/run/netns/$pid# 为container1 创建一对虚拟网卡ip link add name vethC2Host mtu 1500 type veth peer name vethC2Container mtu 1500# veth pair 的一端加入创建的 OVS 网桥中，设置 VLAN 为 2ovs-vsctl add-port ovs-br0 vethC2Host tag=2ip link set vethC2Host up# veth pair 的另一端加入容器所在 namespaceip link set vethC2Container netns $pid# 进入 $pid 所在 netns 中，配置刚刚放入的网卡，改名为 eth0，配置 IP 并启用ip netns exec $pid ip link set dev vethC2Container name eth0ip netns exec $pid ip addr add 192.168.0.2/24 dev eth0ip netns exec $pid ip link set eth0 up container3 和 container4 在 centos2 操作 container3 操作 12345678910111213141516171819202122# 创建容器 --net=none 表示这个容器没有网卡设备，272c3baad162 表示我自己准备的 centos镜像IDdocker run -tid --net=none --name=container3 272c3baad162 /bin/bashpid=$(docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' container3)echo $pid# 在 /var/run/netns 目录下面创建可以使用 ip netns 命令操作的 namespaceln -s /proc/$pid/ns/net /var/run/netns/$pid# 为container1 创建一对虚拟网卡ip link add name vethC3Host mtu 1500 type veth peer name vethC3Container mtu 1500# veth pair 的一端加入创建的 OVS 网桥中，设置 VLAN 为 1ovs-vsctl add-port ovs-br0 vethC3Host tag=1ip link set vethC3Host up# veth pair 的另一端加入容器所在 namespaceip link set vethC3Container netns $pid# 进入 $pid 所在 netns 中，配置刚刚放入的网卡，改名为 eth0，配置 IP 并启用ip netns exec $pid ip link set dev vethC3Container name eth0ip netns exec $pid ip addr add 192.168.0.3/24 dev eth0ip netns exec $pid ip link set eth0 up container4 操作 1234567891011121314151617181920212223# 创建容器 --net=none 表示这个容器没有网卡设备，272c3baad162 表示我自己准备的 centos镜像IDdocker run -tid --net=none --name=container4 272c3baad162 /bin/bashpid=$(docker inspect -f '&#123;&#123;.State.Pid&#125;&#125;' container4)echo $pid# 在 /var/run/netns 目录下面创建可以使用 ip netns 命令操作的 namespaceln -s /proc/$pid/ns/net /var/run/netns/$pid# 为container1 创建一对虚拟网卡ip link add name vethC4Host mtu 1500 type veth peer name vethC4Container mtu 1500# veth pair 的一端加入创建的 OVS 网桥中，设置 VLAN 为 2ovs-vsctl add-port ovs-br0 vethC4Host tag=2ip link set vethC4Host up# veth pair 的另一端加入容器所在 namespaceip link set vethC4Container netns $pid# 进入 $pid 所在 netns 中，配置刚刚放入的网卡，改名为 eth0，配置 IP 并启用ip netns exec $pid ip link set dev vethC4Container name eth0ip netns exec $pid ip addr add 192.168.0.4/24 dev eth0ip netns exec $pid ip link set eth0 up 网络测试 登录容器方法：docker exec -ti container1 bash 同时登录4个容器， container1 和 container3 互相 ping； container2 和 container4 互相 ping；这个是为了更快的让网桥学习 ARP 表。 正常的测试结果 tag 1 vlan 的容器可以互相 ping 通，但是 ping 不通 tag 2 的容器。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[rabbitmq故障修复]]></title>
      <url>%2F2017%2F08%2F23%2Frabbitmq%E6%95%85%E9%9A%9C%E4%BF%AE%E5%A4%8D%2F</url>
      <content type="text"><![CDATA[故障类型 集群不可用 集群网络分区 消息队列卡死 其他 修复操作集群不可用 问题现象： 三个消息队列服务，其中一个或多个无法启动，service rabbitmq-server start 命令无法拉起服务。 处理方法：重建集群 删除集群缓存文件 12rm -f /var/lib/rabbitmq/erl_crash.dump rm -rf /var/lib/rabbitmq/mnesia/* 重启三个消息队列服务 1service rabbitmq-server restart 选一个主节点，创建集群 1rabbitmqctl start_app 其他节点，加入集群 123rabbitmqctl stop_apprabbitmqctl join_cluster rabbit@node2rabbitmqctl start_app 集群网络分区 问题现象： 在三个节点执行 rabbitmqctl cluster_status 命令，看到节点列表不同 处理方法： 重建集群，方法同上 消息队列消息积压 问题现象： 消息队列执行命令非常卡， rabbitmqctl list_queues 等命令都无法正常输出。 某些队列消息非常多，导致消息积压。 处理方法： 清除消息。 1rabbitmqadmin purge queue name=ceilometer.collector.metering 其他 暂时还没有碰到过其他场景，后面碰到再来补充]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[kubernetes学习笔记（一）]]></title>
      <url>%2F2017%2F08%2F21%2Fkubernetes%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
      <content type="text"><![CDATA[基本概念 Pod Pod是一组紧密关联的容器集合，它们共享IPC、Network和UTC namespace，是Kubernetes调度的基本单位。Pod的设计理念是支持多个容器在一个Pod中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 包含多个共享IPC、Network和UTC namespace的容器，可直接通过localhost通信 所有Pod内容器都可以访问共享的Volume，可以访问共享数据 Pod一旦调度后就跟Node绑定，即使Node挂掉也不会重新调度，推荐使用Deployments、Daemonsets等控制器来容错 优雅终止：Pod删除的时候先给其内的进程发送SIGTERM，等待一段时间（grace period）后才强制停止依然还在运行的进程 特权容器（通过SecurityContext配置）具有改变系统配置的权限（在网络插件中大量应用） Namespace Namespace是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的pods, services, replication controllers和deployments等都是属于某一个namespace的（默认是default），而node, persistentVolumes等则不属于任何namespace。 Namespace常用来隔离不同的服务，比如Kubernetes自带的服务一般运行在kube-system namespace中，而其他的服务可以运行在用户指定的命名空间中。例如，研发的命名空间（假设为dev），测试的命名空间（假设为test），运维的命名空间（假设为ops）等等，用户可以自行创建和删除命名空间。 Node Node是Pod真正运行的主机，可以物理机，也可以是虚拟机。为了管理Pod，每个Node节点上至少要运行container runtime（比如docker或者rkt）、kubelet和kube-proxy服务。 不像其他的资源（如Pod和Namespace），Node本质上不是Kubernetes来创建的，Kubernetes只是管理Node上的资源。虽然可以通过Manifest创建一个Node对象（如下json所示），但Kubernetes也只是去检查是否真的是有这么一个Node，如果检查失败，也不会往上调度Pod。 默认情况下，kubelet在启动时会向master注册自己，并创建Node资源。 每个Node都包括以下状态信息 地址：包括hostname、外网IP和内网IP 条件（Condition）：包括OutOfDisk、Ready、MemoryPressure和DiskPressure 容量（Capacity）：Node上的可用资源，包括CPU、内存和Pod总数 基本信息（Info）：包括内核版本、容器引擎版本、OS类型等 Service Service是对一组提供相同功能的Pods的抽象，并为它们提供一个统一的入口。借助Service，应用可以方便的实现服务发现与负载均衡，并实现应用的零宕机升级。Service通过标签来选取服务后端，一般配合Replication Controller或者Deployment来保证后端容器的正常运行。这些匹配标签的Pod IP和端口列表组成endpoints，由kube-proxy负责将服务IP负载均衡到这些endpoints上。 Service有四种类型： ClusterIP：默认类型，自动分配一个仅cluster内部可以访问的虚拟IP NodePort：在ClusterIP基础上为Service在每台机器上绑定一个端口，这样就可以通过 NodeIP:NodePort 来访问该服务 LoadBalancer：在NodePort的基础上，借助cloud provider创建一个外部的负载均衡器，并将请求转发到 NodeIP:NodePort ExternalName：将服务通过DNS CNAME记录方式转发到指定的域名（通过spec.externlName设定）。需要kube-dns版本在1.7以上。 Deployment Deployment为Pod和ReplicaSet提供了一个声明式定义(declarative)方法，用来替代以前的ReplicationController来方便的管理应用。 Deployment典型的应用场景包括： 定义Deployment来创建Pod和ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续Deployment Deployment 为 Pod 和 Replica Set（下一代 Replication Controller）提供声明式更新。 你只需要在Deployment中描述你想要的目标状态是什么，Deployment controller就会帮你将Pod和Replica Set的实际状态改变到你的目标状态。你可以定义一个全新的Deployment，也可以创建一个新的替换旧的Deployment。 Deployment 一个典型的用例如下： 使用Deployment来创建ReplicaSet。ReplicaSet在后台创建pod。检查启动状态，看它是成功还是失败。 然后，通过更新Deployment的PodTemplateSpec字段来声明Pod的新状态。这会创建一个新的ReplicaSet，Deployment会按照控制的速率将pod从旧的ReplicaSet移动到新的ReplicaSet中。 如果当前状态不稳定，回滚到之前的Deployment revision。每次回滚都会更新Deployment的revision。 扩容Deployment以满足更高的负载。 暂停Deployment来应用PodTemplateSpec的多个修复，然后恢复上线。 根据Deployment 的状态判断上线是否hang住了。 清除旧的不必要的ReplicaSet。 Secret Secret解决了密码、token、密钥等敏感数据的配置问题，而不需要把这些敏感数据暴露到镜像或者Pod Spec中。Secret可以以Volume或者环境变量的方式使用。 Secret有三种类型： Opaque：base64编码格式的Secret，用来存储密码、密钥等；但数据也通过base64 –decode解码得到原始数据，所有加密性很弱。 kubernetes.io/dockerconfigjson：用来存储私有docker registry的认证信息。 kubernetes.io/service-account-token： 用于被serviceaccount引用。serviceaccout创建时Kubernetes会默认创建对应的secret。Pod如果使用了serviceaccount，对应的secret会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中。 备注： serviceaccount用来使得Pod能够访问Kubernetes API StatefulSet StatefulSet是为了解决有状态服务的问题（对应Deployments和ReplicaSets是为无状态服务而设计）。 StatefulSet其应用场景包括 稳定的持久化存储，即Pod重新调度后还是能访问到相同的持久化数据，基于PVC来实现 稳定的网络标志，即Pod重新调度后其PodName和HostName不变，基于Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即Pod是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依序进行（即从0到N-1，在下一个Pod运行之前所有之前的Pod必须都是Running和Ready状态），基于init containers来实现 有序收缩，有序删除（即从N-1到0） 从上面的应用场景可以发现，StatefulSet由以下几个部分组成： 用于定义网络标志（DNS domain）的Headless Service 用于创建PersistentVolumes的volumeClaimTemplates 定义具体应用的StatefulSet DaemonSet DaemonSet保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。 典型的应用包括： 日志收集，比如fluentd，logstash等 系统监控，比如Prometheus Node Exporter，collectd，New Relic agent，Ganglia gmond等 系统程序，比如kube-proxy, kube-dns, glusterd, ceph等 Service Account Service account是为了方便Pod里面的进程调用Kubernetes API或其他外部服务而设计的。 Service Account 与 User account 不同 User account是为人设计的，而service account则是为Pod中的进程调用Kubernetes API而设计； User account是跨namespace的，而service account则是仅局限它所在的namespace； 每个namespace都会自动创建一个default service account Token controller检测service account的创建，并为它们创建secret 开启ServiceAccount Admission Controller后 每个Pod在创建后都会自动设置spec.serviceAccount为default（除非指定了其他ServiceAccout） 验证Pod引用的service account已经存在，否则拒绝创建 如果Pod没有指定ImagePullSecrets，则把service account的ImagePullSecrets加到Pod中 每个container启动后都会挂载该service account的token和ca.crt到/var/run/secrets/kubernetes.io/serviceaccount/ ReplicationController和ReplicaSet ReplicationController（也简称为rc）用来确保容器应用的副本数始终保持在用户定义的副本数，即如果有容器异常退出，会自动创建新的Pod来替代；而异常多出来的容器也会自动回收。ReplicationController的典型应用场景包括确保健康Pod的数量、弹性伸缩、滚动升级以及应用多版本发布跟踪等。 在新版本的Kubernetes中建议使用ReplicaSet（也简称为rs）来取代ReplicationController。ReplicaSet跟ReplicationController没有本质的不同，只是名字不一样，并且ReplicaSet支持集合式的selector（ReplicationController仅支持等式）。 虽然也ReplicaSet可以独立使用，但建议使用 Deployment 来自动管理ReplicaSet，这样就无需担心跟其他机制的不兼容问题（比如ReplicaSet不支持rolling-update但Deployment支持），并且还支持版本记录、回滚、暂停升级等高级特性。 Job Job负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。 Kubernetes支持以下几种Job： 非并行Job：通常创建一个Pod直至其成功结束 固定结束次数的Job：设置.spec.completions，创建多个Pod，直到.spec.completions个Pod成功结束 带有工作队列的并行Job：设置.spec.Parallelism但不设置.spec.completions，当所有Pod结束并且至少一个成功时，Job就认为是成功 根据.spec.completions和.spec.Parallelism的设置，可以将Job划分为以下几种pattern： Job类型 使用示例 行为 completions Parallelism 一次性Job 数据库迁移 创建一个Pod直至其成功结束 1 1 固定结束次数的Job 处理工作队列的Pod 依次创建一个Pod运行直至completions个成功结束 2+ 1 固定结束次数的并行Job 多个Pod同时处理工作队列 依次创建多个Pod运行直至completions个成功结束 2+ 2+ 并行Job 多个Pod同时处理工作队列 创建一个或多个Pod直至有一个成功结束 1 2+ Horizontal Pod Autoscaling（HPA） Horizontal Pod Autoscaling可以根据CPU使用率或应用自定义metrics自动扩展Pod数量（支持replication controller、deployment和replica set）。 控制管理器每隔30s（可以通过–horizontal-pod-autoscaler-sync-period修改）查询metrics的资源使用情况 支持三种metrics类型 预定义metrics（比如Pod的CPU）以利用率的方式计算 自定义的Pod metrics，以原始值（raw value）的方式计算 自定义的object metrics 支持两种metrics查询方式：Heapster和自定义的REST API 支持多metrics Network Policy 随着微服务的流行，越来越多的云服务平台需要大量模块之间的网络调用。Kubernetes 在 1.3 引入了Network Policy，Network Policy提供了基于策略的网络控制，用于隔离应用并减少攻击面。它使用标签选择器模拟传统的分段网络，并通过策略控制它们之间的流量以及来自外部的流量。 Ingress 通常情况下，service和pod的IP仅可在集群内部访问。集群外部的请求需要通过负载均衡转发到service在Node上暴露的NodePort上，然后再由kube-proxy通过边缘路由器(edge router)将其转发给相关的Pod或者丢弃。 而Ingress就是为进入集群的请求提供路由规则的集合。ngress可以给service提供集群外部访问的URL、负载均衡、SSL终止、HTTP路由等。为了配置这些Ingress规则，集群管理员需要部署一个Ingress controller，它监听Ingress和service的变化，并根据规则配置负载均衡并提供访问入口。 ConfigMap ConfigMap用于保存配置数据的键值对，可以用来保存单个属性，也可以用来保存配置文件。ConfigMap跟secret很类似，但它可以更方便地处理不包含敏感信息的字符串。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[ansible的setup模块可能导致进程卡死]]></title>
      <url>%2F2017%2F01%2F07%2Fansible-setup-module-problem-md%2F</url>
      <content type="text"><![CDATA[问题背景 概述 一次 playbook 的测试过程中，发现一个奇怪的现象：就是执行 playbook 的过程中，总会出现一个 setup 的步骤，并且这个步骤执行的时间非常的长，达到5分钟，甚至有的环境会直接卡死，进程无法中断。详细的问题描述见github-ansible 环境介绍 通过ansible节点管理openstack环境,所以环境中有很多虚拟网卡，导致setup模块的问题。 问题现象 test.yaml 文件 12345---- hosts: xx.xx.xx.xx tasks: - name: test shell: echo aaaa 执行之后，直接卡死，就像下面这样，一直没有返回。 12345# ansible-playbook test.yamlPLAY [xx.xx.xx.xx] ************************************************************TASK [setup] ******************************************************************* 问题分析分别到ansible节点和目标节点查看原因。 ansible节点 1237901 pts/2 Sl+ 2:43 \_ /usr/bin/python /usr/bin/ansible all -m setup7912 pts/2 S+ 0:00 \_ /usr/bin/python /usr/bin/ansible all -m setup7960 pts/2 S+ 0:00 \_ ssh -C -o ControlMaster=auto -o ControlPersist=60s -o KbdInteractiveAuthentication=no -o PreferredAuthentications=gssapi-with-mic,gssapi-keyex,hostbased,publickey -o PasswordAuthentication=no -o ConnectTimeout=10 -o ControlPath=/root/.ansible/cp/ansible-ssh-%h-%p-%r -tt xx.xx.xx.xx /bin/sh -c '/usr/bin/python /root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0' 目标节点 12349036 pts/1 Ss+ 0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0 49083 pts/1 S+ 0:00 \_ /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483608708.39-143999004080315/setup.py 49213 pts/1 D+ 0:00 \_ /usr/bin/python /tmp/ansible_bx9h6K/ansible_module_setup.py 请注意，这里进程进入了 D 状态，这个状态的意思是不可中断的进程，会一直卡着，无法返回。 setup模块 既然问题出在 setup 模块，那就先来看下setup 模块是干什么的，通过查看源码和文档，这个是获取系统信息，并将相关变量存储到facts中，具体的就不在这里展开分析了。 其中获取网卡信息，过程大概如下123456789101180013 pts/2 Ss+ 0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0 80034 pts/2 S+ 0:00 \_ /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py 80073 pts/2 S+ 0:00 \_ /usr/bin/python /tmp/ansible_pBITNP/ansible_module_setup.py 80092 pts/2 Sl+ 0:25 \_ /usr/bin/ruby /usr/bin/facter --puppet --json 15676 pts/2 R+ 0:00 \_ /sbin/ip link show ovs-system 80013 pts/2 Ss+ 0:00 /bin/sh -c /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py; rm -rf "/root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/" &gt; /dev/null 2&gt;&amp;1 &amp;&amp; sleep 0 80034 pts/2 S+ 0:00 \_ /usr/bin/python /root/.ansible/tmp/ansible-tmp-1483615192.93-195988658363074/setup.py 80073 pts/2 S+ 0:00 \_ /usr/bin/python /tmp/ansible_pBITNP/ansible_module_setup.py 80092 pts/2 Sl+ 0:25 \_ /usr/bin/ruby /usr/bin/facter --puppet --json 12971 pts/2 R+ 0:00 \_ /sbin/ip link show ifb992 问题处理openstack的网络节点，会有非常多的虚拟网卡，我的这个环境中虚拟网卡个数接近1000个了，导致收集信息的时间长。既然只是获取信息，那就能否跳过这一个步骤呢？当然是有的，修改配置文件 /etc/ansible/ansible.cfg，其中有一个配置项 gather_subset 1234#原值：#gather_subset=all#新值：gather_subset=!all 修改之后，再次执行playbook，问题得到解决。具体其他的配置项，可以查看setup的源码。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[helloblog]]></title>
      <url>%2F2016%2F12%2F07%2Fhelloblog%2F</url>
      <content type="text"><![CDATA[博客终于开张了！一直想整一个自己的技术博客，最近终于抽出时间整起来了，感谢gitbub,hexo等相关技术贡献者，给大家带来的免费福利，搭建博客的过程很简单。 为什么要搭建自己的博客？ 为了更好的记录和分享 结交更多志同道合的朋友 表达自己最真实的想法 坚持写作，表达，能更好的理解相关内容 虽然以前也有在 CSDN博客 上写过几篇，但是没有坚持下来（当然也有其他的一些原因），这次打算坚持写下去。以前文笔不好，也许写者写着就好起来了呢；以前不喜欢记录和分享，主要还是太懒了，好记性不如烂笔头，自己写出来的东西才是自己理解的，没有明白这个道理，始终只能在入门阶段徘徊，想要成为一代宗师，自己的总结和思考必不可少。 这并不仅仅只是一个技术博客，也许偶尔也会发发牢骚。 好了，写的有点乱，后面坚持写自己的博客！ 最后，以前的 CSDN博客 不再更新，相关文章也会在近期转移到本博客。]]></content>
    </entry>

    
  
  
</search>
